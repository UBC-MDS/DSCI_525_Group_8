{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "114cf9c2",
   "metadata": {},
   "source": [
    "# DSCI 525 - Web and Cloud Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcfdb0d",
   "metadata": {},
   "source": [
    "Milestone 2: Your team is planning to migrate to the cloud. AWS gave 400$ (100$ each) to your team to support this. As part of this initiative, your team needs to set up a server in the cloud, a collaborative environment for your team, and later move your data to the cloud. After that, your team can wrangle the data in preparation for machine learning.\n",
    "\n",
    "## Milestone 2 checklist  \n",
    "You will have mainly 2 tasks. Here is the checklist...\n",
    "- Task 1: To set up a collaborative environment \n",
    "    - Setup your EC2 instance with JupyterHub.\n",
    "    - Install all necessary things needed in your UNIX server (Amazon EC2 instance).\n",
    "    - Set up your S3 bucket.\n",
    "    - Move the data that you wrangled in your last milestone to S3.\n",
    "- Task 2: Wrangle the data in preparation for machine learning\n",
    "    - Get the data from S3 in your notebook and make data ready for machine learning.\n",
    "\n",
    "> Everything in this milestone is to be completed in sequential order. But if you want to divide the tasks, you can ask a team member to work on Task 2 (```4. Get the data that we wrangled in our first milestone.``` and ```6. Wrangle the data in preparation for machine learning```) locally on their laptop while the other members set up the infrastructure (EC2, S3) and TLJH in the cloud. This way, you can move quick.\n",
    "\n",
    "_***Outside of Milestone:*** I strongly recommend you spin up your own instance and experiment with the s3 bucket in doing something (there are many things that we learned and practical work from video series) to get comfortable with AWS. But we won't be looking at it for a grading purpose._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6849a9a",
   "metadata": {},
   "source": [
    "**Keep in mind:**\n",
    "\n",
    "- _All services you use are in region ```us-west-2```._\n",
    "\n",
    "- _Don't store anything in these servers or storage that represents your identity as a student (like your student ID number) ._\n",
    "\n",
    "- _Use only default VPC and subnet._\n",
    "    \n",
    "- _No IP addresses are visible when you provide the screenshot._\n",
    "\n",
    "- _You do proper budgeting so that you don't run out of credits._ \n",
    "\n",
    "- _We want one single notebook for grading, and it's up to your discretion on how you do it. ***So only one person in your group needs to spin up instance and a ```t2.large``` is of decent size.***_\n",
    "\n",
    "- _Please stop the instance when not in use. This can save you some bucks, but it's again up to you and how you budget your money. Maybe stop it if you or your team won't use it for the next 5 hours?_\n",
    "\n",
    "- _Your AWS lab will shut down after 4 hours (also your services). When you start it again (after it is stopped), your AWS credentials (***access key***,***secret***, and ***session token***) will change, and you want to update your credentials file with the new one._\n",
    "\n",
    "- _If you don't want your lab to shut down, click on the start lab again before 4 hours ends. This will fill up your time to 4 hours._\n",
    "\n",
    "- _Say something went wrong and you want to spin up another EC2 instance, then it will be good to terminate the previous one._\n",
    "\n",
    "- _We will be choosing the storage to be ```Delete on Termination``` ( that is the default option), which means that stored data in your instance will be lost upon termination. Make sure you save any data to S3 and download the notebooks to your laptop so that next time you have your jupyterHub in a different instance, you can upload your notebook there._\n",
    "\n",
    "- _Wherever I ask for screenshots, you can provide the screenshot location in your GitHub._\n",
    "\n",
    "***NOTE:*** Everything you want for this notebook is discussed in lecture 3 and lecture 4. So you can follow the same order of things shown in lecture 4. You can also refer to the videos linked in each section under image ![YouTube](https://cdn.emojidex.com/emoji/hdpi/YouTube.png \"YouTube\") to see a demo on how to do it. In addition, I will put a link to the lecture note section that applies to each question in this milestone so that you don't get lost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1884cb4",
   "metadata": {},
   "source": [
    "### 1. Setup your EC2 instance\n",
    "- Name of the instance to be `mds-your_groupnumber`. For example if my group is 14 I would name it `mds-14`.\n",
    "- AMI to be `Ubuntu server 22.04 LTS (HVM)`.\n",
    "- Instance type to be `t2.large`.\n",
    "- Architecture to be `64-bit(x86)`.\n",
    "- Storage to be `30 GB`.\n",
    "- Make sure you install TLJH in your instance, by giving instructions in `User Data`.\n",
    "\n",
    "> Check [this](https://pages.github.ubc.ca/MDS-2022-23/DSCI_525_web-cloud-comp_students/lectures/lecture4.html#setting-up-an-ec2-instance) section in lecture notes for more details on setting up EC2 instance with TLJH."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e80c45",
   "metadata": {},
   "source": [
    "rubric={correctness:20}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e978e34",
   "metadata": {},
   "source": [
    "**Github path to screenshot: https://github.com/UBC-MDS/DSCI_525_Group_8/blob/main/milestone2/image/Q1_EC2-instance-screenshot.png**\n",
    "\n",
    "<img src=\"image/Q1_EC2-instance-screenshot.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9801b911",
   "metadata": {},
   "source": [
    "### 2. Setup the server \n",
    "\n",
    "rubric={correctness:20}\n",
    "\n",
    "2.1) Add your team members to EC2 instance.\n",
    "\n",
    "2.2) Setup a common data folder to download data, and this folder should be accessible by all users in the JupyterHub.\n",
    "    \n",
    "2.3) Install and configure AWS CLI.\n",
    "\n",
    "> Check following sections in lecture notes for more details;\n",
    "\n",
    "- [Logging into EC2s](https://pages.github.ubc.ca/MDS-2022-23/DSCI_525_web-cloud-comp_students/lectures/lecture4.html#logging-into-ec2)\n",
    "- [Setting up a common space in EC2](https://pages.github.ubc.ca/MDS-2022-23/DSCI_525_web-cloud-comp_students/lectures/lecture4.html#setting-up-a-common-space-in-ec2) \n",
    "- [AWS CLI](https://pages.github.ubc.ca/MDS-2022-23/DSCI_525_web-cloud-comp_students/lectures/lecture3.html#aws-cli)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c732ef2d",
   "metadata": {},
   "source": [
    "**GitHub path to screenshot: https://github.com/UBC-MDS/DSCI_525_Group_8/blob/main/milestone2/image/Q2_CLI_access.jpg**\n",
    "\n",
    "<img src=\"image/Q2_CLI_access.jpg\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ad5a3",
   "metadata": {},
   "source": [
    "### 3. Setup your JupyterHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f8283",
   "metadata": {},
   "source": [
    "rubric={correctness:20}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2dea6704",
   "metadata": {},
   "source": [
    "**GitHub path to your screenshot: https://github.com/UBC-MDS/DSCI_525_Group_8/blob/main/milestone2/image/Q3_jupyter-hub-screenshot.png**\n",
    "\n",
    "<img src=\"image/Q3_jupyter-hub-screenshot.png\" >\n",
    "\n",
    "<br></br>\n",
    "> Check [this](https://pages.github.ubc.ca/MDS-2022-23/DSCI_525_web-cloud-comp_students/lectures/lecture4.html#setup-your-jupyterhub) section in lecture notes for more details on setting up jupyterHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeecde21",
   "metadata": {},
   "source": [
    "### 4. Get the data what we wrangled in our first milestone. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2174bec3",
   "metadata": {},
   "source": [
    "You have to install the packages that are needed. Refer this TLJH [document]( https://tljh.jupyter.org/en/latest/howto/env/user-environment.html).Refer ```pip``` section.\n",
    "\n",
    "Don't forget to add option -E. This way, all packages that you install will be available to other users in your JupyterHub.\n",
    "These packages you must install and install other packages needed for your wrangling.\n",
    "\n",
    "    sudo -E pip install pandas\n",
    "    sudo -E pip install pyarrow\n",
    "    sudo -E pip install s3fs\n",
    "\n",
    "> Check [this](https://pages.github.ubc.ca/MDS-2022-23/DSCI_525_web-cloud-comp_students/lectures/lecture4.html#how-to-install-packages-in-tljh) section in lecture notes for more details on installing packages in TLJH."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d32fbd",
   "metadata": {},
   "source": [
    "As in the last milestone, we looked at transferring the data from Python to R, and we have different solutions. To stay consistent, I uploaded the parquet file (which I combined and converted in the last milestone), which we can use moving forward. Here in this section, I am getting that file using API; (the section is prepopulated for you)\n",
    "\n",
    "Remember, from now on, we are going to run this notebook in the TLJH that is set up from previous steps. You can upload your notebook there using the upload button in jupyter. After that, you are going to run the following code to get the data downloaded to your EC2 instance and then you move to questions 5 and 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6a16f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.4) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b16325",
   "metadata": {},
   "source": [
    "Rememeber here we gave the folder that we created in Step 2.2 as we made it available for all the users in a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07bd98c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary metadata\n",
    "article_id = 14226968  # this is the unique identifier of the article on figshare\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"/srv/data/my_shared_data_folder/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dae876e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'is_link_only': False,\n",
       "  'name': 'allyears.csv.zip',\n",
       "  'supplied_md5': '9e046ac05ecd2c32a256a47dd1098b81',\n",
       "  'computed_md5': '9e046ac05ecd2c32a256a47dd1098b81',\n",
       "  'id': 26844650,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26844650',\n",
       "  'size': 2405908113},\n",
       " {'is_link_only': False,\n",
       "  'name': 'individual_years.zip',\n",
       "  'supplied_md5': '921da748974b07b2a70bbfcc04535a77',\n",
       "  'computed_md5': '921da748974b07b2a70bbfcc04535a77',\n",
       "  'id': 26863682,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26863682',\n",
       "  'size': 1896206676},\n",
       " {'is_link_only': False,\n",
       "  'name': 'combined_model_data.csv.zip',\n",
       "  'supplied_md5': '7638434c44a7d29cbb29fe200b4fd65d',\n",
       "  'computed_md5': '7638434c44a7d29cbb29fe200b4fd65d',\n",
       "  'id': 27515426,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/27515426',\n",
       "  'size': 821308997},\n",
       " {'is_link_only': False,\n",
       "  'name': 'combined_model_data_parti.parquet.zip',\n",
       "  'supplied_md5': '02f4e3df8d16580a02291de225072689',\n",
       "  'computed_md5': '02f4e3df8d16580a02291de225072689',\n",
       "  'id': 27520682,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/27520682',\n",
       "  'size': 519743915},\n",
       " {'is_link_only': False,\n",
       "  'name': 'combined_model_data.parquet',\n",
       "  'supplied_md5': 'ae63699ab21ffa8006559c6afbcd2271',\n",
       "  'computed_md5': 'ae63699ab21ffa8006559c6afbcd2271',\n",
       "  'id': 27520808,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/27520808',\n",
       "  'size': 565872005}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)  # this contains all the articles data, feel free to check it out\n",
    "files = data[\"files\"]             # this is just the data about the files, which is what we want\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55ae9ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_dl = [\"combined_model_data_parti.parquet.zip\"]  ## Please download the partitioned \n",
    "for file in files:\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11642663",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(os.path.join(output_directory, \"combined_model_data_parti.parquet.zip\"), 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2f04d9",
   "metadata": {},
   "source": [
    "### 5. Setup your S3 bucket and move data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c2bbf",
   "metadata": {},
   "source": [
    "rubric={correctness:15}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc337efd",
   "metadata": {},
   "source": [
    "> Check [this](https://pages.github.ubc.ca/MDS-2022-23/DSCI_525_web-cloud-comp_students/lectures/lecture4.html#setting-up-s3-bucket) section in lecture notes for more details on setting up S3 bucket and ***making it public***. \n",
    "\n",
    "5.1)  Create a bucket and name should be mds-s3-groupnumber-studentname. Replace groupnumber with your \"group number\", and studentname with your \"student name\". For example, if my group is 14 and my name is \"Gittu George\", then I would name it \"mds-s3-14-gittu\". (Note: For the purpose of this milestone only one student need to create this.)\n",
    "\n",
    "5.2)  Create your first folder called \"output\".\n",
    "\n",
    "5.3) Move the \"observed_daily_rainfall_SYD.csv\" file from the Milestone1 data folder to your s3 bucket from your local computer.\n",
    "\n",
    "5.4) Moving the parquet file we downloaded(combined_model_data_parti.parquet) in step 4 to S3 using the cli what we installed in step 2.3.\n",
    "\n",
    "\n",
    "> Check [this](https://pages.github.ubc.ca/MDS-2022-23/DSCI_525_web-cloud-comp_students/lectures/lecture4.html#how-to-transfer-data-to-s3) section in lecture notes for more details on moving data to S3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66d3e08c",
   "metadata": {},
   "source": [
    "**GitHub path to your screenshot for grading: https://github.com/UBC-MDS/DSCI_525_Group_8/blob/main/milestone2/image/Q4_S2_files.png**\n",
    "\n",
    "Make sure it has 3 objects and see red `public` symbol in your bucket. Refer below screenshot for reference.\n",
    "\n",
    "<img src=\"image/Q4_S2_files.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d40246",
   "metadata": {},
   "source": [
    "### 6. Wrangle the data in preparation for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ca7b4",
   "metadata": {},
   "source": [
    "rubric={correctness:20}\n",
    "\n",
    "> ***IMPORTANT:*** Don't forget to deal with the credentials before you start this section. Check details in lecture notes [here](https://pages.github.ubc.ca/MDS-2022-23/DSCI_525_web-cloud-comp_students/lectures/lecture4.html#more-about-credentials)\n",
    "\n",
    "> Check [this](https://pages.github.ubc.ca/MDS-2022-23/DSCI_525_web-cloud-comp_students/lectures/lecture4.html#how-to-read-data-from-s3) section in lecture notes for more details on how to read data from S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f1fca0",
   "metadata": {},
   "source": [
    "Our data currently covers all of NSW, but say that our client wants us to create a machine learning model to predict rainfall over Sydney only. There's a bit of wrangling that needs to be done for that:\n",
    "1. We need to query our data for only the rows that contain information covering Sydney\n",
    "2. We need to wrangle our data into a format suitable for training a machine learning model. That will require pivoting, resampling, grouping, etc.\n",
    "\n",
    "To train an ML algorithm we need it to look like this:\n",
    "\n",
    "||model-1_rainfall|model-2_rainfall|model-3_rainfall|...|observed_rainfall|\n",
    "|---|---|---|---|---|---|\n",
    "|0|0.12|0.43|0.35|...|0.31|\n",
    "|1|1.22|0.91|1.68|...|1.34|\n",
    "|2|0.68|0.29|0.41|...|0.57|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b4929d",
   "metadata": {},
   "source": [
    "6.1) Get the data from s3 (```combined_model_data_parti.parquet``` and ```observed_daily_rainfall_SYD.csv```)\n",
    "\n",
    "6.2) First query for Sydney data and then drop the lat and lon columns (we don't need them).\n",
    "```\n",
    "syd_lat = -33.86\n",
    "syd_lon = 151.21\n",
    "```\n",
    "Expected shape ```(1150049, 2)```.\n",
    "\n",
    "6.3) Save this processed file to s3 for later use:\n",
    "\n",
    "  Save as a csv file ```ml_data_SYD.csv``` to ```s3://mds-s3-xxx/output/```\n",
    "  expected shape ```(46020,26)``` - This includes all the models as columns and also adding additional column ```Observed``` loaded from ```observed_daily_rainfall_SYD.csv``` from s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04d572df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do all your coding here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d07de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.parse\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a6d05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"s3://mds-s3-8-mehwish/observed_daily_rainfall_SYD.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b98047ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lisaa\\UBC_Lab_Block_6\\DSCI_525_cloud_comp\\figshare\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "aws_credentials ={\"key\": \"<my_key>\",\n",
    "                  \"secret\": \"<.y_secret>\",\n",
    "                  \"token\":\"<my_token>\"} \n",
    "## dont include you secret and key when submitting the notebook\n",
    "df2 = pd.read_parquet(\"s3://mds-s3-8-mehwish/combined_model_data_parti.parquet\", storage_options=aws_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bc399",
   "metadata": {},
   "outputs": [],
   "source": [
    "syd_lat = -33.86\n",
    "syd_lon = 151.21\n",
    "\n",
    "# create masks for latitudes and longitudes\n",
    "lat_mask = (df2['lat_min'] <= syd_lat) & (df2['lat_max'] >= syd_lat)\n",
    "lon_mask = (df2['lon_min'] <= syd_lon) & (df2['lon_max'] >= syd_lon)\n",
    "\n",
    "# combine the masks using logical AND to get the final mask\n",
    "sydney_mask = lat_mask & lon_mask\n",
    "\n",
    "# filter the dataframe using the final mask\n",
    "sydney_df = df2[sydney_mask].drop(columns = ['lat_min', 'lat_max','lon_min', 'lon_max'])\n",
    "sydney_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72c114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sydney_model_df = pd.merge(df, sydney_df, on=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fbbce5",
   "metadata": {},
   "source": [
    "How the final file format looks like\n",
    "<img src=\"image/finaloutput.png\" >\n",
    "\n",
    "Shape ```(46020,26 )```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21891d4d",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Submission instructions\n",
    "rubric={mechanics:5}\n",
    "\n",
    "In the textbox provided on Canvas for the Milestone 2 assignment include:\n",
    "\n",
    "- The GitHub URL to your Milestone 2 folder.\n",
    "\n",
    "In this milestone 2 folder, you should have this notebook and screenshots asked in this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('xxx10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f75bc37be52f1a54b499f2bd249d6269a196b34b202e3f84ac376713198385a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
