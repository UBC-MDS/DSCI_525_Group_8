{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 (Guided Exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of Milestone 3, Question 4, and is a guided exercise. I have included guidelines and helpful links (as comments) along with this notebook to guide you through the exercise. Daniel's tutorial will also be helpful, but even if you haven't completed it, you should be able to finish this exercise with the help of the links and guidelines provided here. \n",
    "\n",
    "For some of you, this may be the first time exploring a package independently using documentation, so it might take some time to get used to it. But with practice, you will get better at it. At work, you may come across many packages that you have never used before, and you will have to learn them on your own. So, this is a good practice to get used to it.\n",
    "\n",
    "In this exercise, you will use Spark's MLlib. The idea is to tune some hyperparameters of a Random Forest to find the optimum model. Once you know the optimum settings, you will train a Random Forest in sklearn (`Milestone3-task3.ipynb` part 2) and save it with joblib (`Milestone3-task3.ipynb` part 2) so that you can use it next week to deploy.\n",
    "\n",
    "Here, consider MLlib as another Python package that you are using, like scikit-learn. You will see many scikit-learn similar classes and methods available in MLlib for various ML-related tasks. You may also notice that some of them are not yet implemented in MLlib. What you write using the PySpark package will use the spark engine to run your code, and hence, all the benefits of distributed computing that we discussed in class.\n",
    "\n",
    "Note: Whenever you use Spark, make sure that you refer to the right documentation based on the version you are using. You can select the version of Spark from [here](https://spark.apache.org/docs/)  and go to the correct documentation. In our case, we are using Spark 3.3.1, and here is the link to the Spark documentation that you can refer to:\n",
    "\n",
    "- [MLlib Documentation](https://spark.apache.org/docs/3.3.1/ml-guide.html)\n",
    "- [MLlib API Reference](https://spark.apache.org/docs/3.3.1/api/python/reference/pyspark.ml.html)\n",
    "\n",
    "You may notice that there are RDD-based API and DataFrame-based (Main Guide) API available in the documentation. You want to focus on DataFrame-based API as no one uses RDD-based API these days. We will discuss the difference in class.\n",
    "\n",
    "Before you start this notebook, make sure that you setup your EMR notebooks, uploaded this notebook there, and the kernel you selected is PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages\n",
    "\n",
    "You only want to install following packages for this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"pandas\")\n",
    "sc.install_pypi_package(\"s3fs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, UnivariateFeatureSelector\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor as sparkRFR\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with; read 100 data points for development purpose. Once your code is ready you should try on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remember by default it looks for credentials in home directory. \n",
    "## Makes sure your updated credentials are in home directory\n",
    "## Where this notebook is running? Clue: It is running on our master node. So you want to ssh into master node and update credentials there.\n",
    "## or pass credentials explicitly and pass as storage_options=aws_credentials (not a good idea)\n",
    "# aws_credentials = {\"key\": \"\",\"secret\": \"\",\"token\":\"\"}\n",
    "# replace with s3 path to your data\n",
    "## here 100 data points for testing the code, \n",
    "pandas_df = pd.read_csv(\"s3://xxxx/output/ml_data_SYD.csv\", index_col=0, parse_dates=True).iloc[:100].dropna()\n",
    "# pandas_df = pd.read_csv(\"s3://xxxx/output/ml_data_SYD.csv\", index_col=0, parse_dates=True).dropna()\n",
    "feature_cols = list(pandas_df.drop(columns=\"Observed\").columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe and coerce features into a single column called \"Features\"\n",
    "# This is a requirement of MLlib\n",
    "# Here we are converting your pandas dataframe to a spark dataframe, \n",
    "# Here \"spark\" is a spark session I will discuss in class. \n",
    "# It is automatically created for you in this notebook.\n",
    "# read more  here https://blog.knoldus.com/spark-createdataframe-vs-todf/\n",
    "training = spark.createDataFrame(pandas_df)\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"Features\")\n",
    "training = assembler.transform(training).select(\"Features\", \"Observed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best hyperparameter settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Official Documentation of MLlib, Random forest regression [here](https://spark.apache.org/docs/3.3.1/ml-classification-regression.html#random-forest-regression).\n",
    "\n",
    "Here we will be mainly using following classes and methods;\n",
    "\n",
    "- [RandomForestRegressor](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.RandomForestRegressor.html)\n",
    "- [ParamGridBuilder](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.ParamGridBuilder.html)\n",
    "    - addGrid\n",
    "    - build\n",
    "- [CrossValidator](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html)\n",
    "    - fit\n",
    "\n",
    "Use these parameters for coming up with ideal parameters, you could try more parameters, but make sure you have enough power to do it. But you are required to try only following parameters. This will take around 15 min on entire dataset....\n",
    "\n",
    "    - Use numTrees as [10, 50,100]\n",
    "    - maxDepth as [5, 10]\n",
    "    - bootstrap as [False, True]\n",
    "    - In the CrossValidator use evaluator to be `RegressionEvaluator(labelCol=\"Observed\")`\n",
    "    \n",
    "***Additional reference:*** You can refer to [here](https://www.sparkitecture.io/machine-learning/regression/random-forest) and [here](https://www.silect.is/blog/random-forest-models-in-spark-ml/).\n",
    "\n",
    "Some additional reading [here](https://projector-video-pdf-converter.datacamp.com/14989/chapter4.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Once you finish testing the model on 100 data points, then load entire dataset and run , this could take ~15 min.\n",
    "## write code below.\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print run info\n",
    "# cvModel is a variable that stores the best model obtained after performing cross-validation (crossval.fit(training))\n",
    "print(\"\\nBest model\")\n",
    "print(\"==========\")\n",
    "print(f\"\\nCV Score: {min(cvModel.avgMetrics):.2f}\")\n",
    "print(f\"n_estimators: {cvModel.bestModel.getNumTrees}\")\n",
    "print(f\"max_depth: {cvModel.bestModel.getMaxDepth()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('jb2023')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "python3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "6e9e0baa62560f8a3b402c12d339bdad33c58a25305700ec7e7682c0b6251f68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
